{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the grid world enviroment\n",
    "grid_size = 5\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "goal_state = (4,4)\n",
    "gamma = 0.9\n",
    "threshold = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[['down' 'down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'right' 'right' 'goal']]\n"
     ]
    }
   ],
   "source": [
    "#initialize value functions\n",
    "\n",
    "values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# define rewards\n",
    "rewards = np.full((grid_size, grid_size), -1)\n",
    "rewards[goal_state] = 10\n",
    "\n",
    "# value iteration\n",
    "\n",
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_values = np.zeros((grid_size, grid_size))\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if (i,j) == goal_state:\n",
    "                    new_values[i, j] = 0\n",
    "                    continue\n",
    "                #calculate the value for each action\n",
    "                action_Values = []\n",
    "                for action in actions:\n",
    "                    if action == 'up':\n",
    "                        next_i, next_j = max(i-1, 0), j\n",
    "                    elif action == 'down':\n",
    "                        next_i, next_j = min(i+1, grid_size-1), j\n",
    "                    elif action == 'left':\n",
    "                        next_i, next_j = i, max(j-1, 0)\n",
    "                    elif action == 'right':\n",
    "                        next_i, next_j = i, min(j+1, grid_size - 1)\n",
    "                    action_Values.append(rewards[next_i, next_j] + gamma * values[next_i, next_j])\n",
    "\n",
    "\n",
    "                #update the value for the current state\n",
    "                new_values[i, j] = max(action_Values)\n",
    "                delta = max(delta, abs(new_values[i, j] - values[i, j]))\n",
    "        values[:] = new_values\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "# Extract the optimal policy\n",
    "def get_optimal_policy():\n",
    "    policy = np.empty((grid_size, grid_size), dtype=object)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if (i, j) == goal_state:\n",
    "                policy[i, j] = 'goal'\n",
    "                continue\n",
    "            action_values = []\n",
    "            for action in actions:\n",
    "                if action == 'up':\n",
    "                    next_i, next_j = max(i - 1, 0), j\n",
    "                elif action == 'down':\n",
    "                    next_i, next_j = min(i + 1, grid_size - 1), j\n",
    "                elif action == 'left':\n",
    "                    next_i, next_j = i, max(j - 1, 0)\n",
    "                elif action == 'right':\n",
    "                    next_i, next_j = i, min(j + 1, grid_size - 1)\n",
    "                action_values.append(rewards[next_i, next_j] + gamma * values[next_i, next_j])\n",
    "            policy[i, j] = actions[np.argmax(action_values)]\n",
    "    return policy\n",
    "\n",
    "# Run value iteration and print results\n",
    "value_iteration()\n",
    "optimal_policy = get_optimal_policy()\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-learning algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world environment\n",
    "grid_size = 5\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "goal_state = (4, 4)  # Bottom-right corner\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.5  # Learning rate\n",
    "num_episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Define rewards\n",
    "rewards = np.full((grid_size, grid_size), -1)\n",
    "rewards[goal_state] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "[[[-3.36579569 -0.434062   -3.36579569 -3.30712321]\n",
      "  [-3.01662704 -2.75268281 -2.90306621 -2.23265769]\n",
      "  [-2.26219063  0.07293168 -2.38447641 -2.16634297]\n",
      "  [-1.8549375  -0.7093125  -1.7361875  -1.545     ]\n",
      "  [-1.42625    -1.2125     -1.7361875  -1.42625   ]]\n",
      "\n",
      " [[-2.8193904  -2.68523117 -2.64908109  0.62882   ]\n",
      "  [-2.52370531 -2.23956563 -2.20562813  1.8098    ]\n",
      "  [-1.90356563 -1.8659375  -1.78428125  3.122     ]\n",
      "  [-1.325      -1.20125    -1.18875     4.58      ]\n",
      "  [-0.975       6.2        -1.18875    -0.975     ]]\n",
      "\n",
      " [[-2.5297782  -2.11303125 -2.26219063 -2.3236875 ]\n",
      "  [-2.04756562 -1.81025    -2.20562813 -2.018375  ]\n",
      "  [-1.977225   -1.539375   -1.787375   -1.60125   ]\n",
      "  [-0.86023437 -1.1        -1.2        -0.975     ]\n",
      "  [-0.975       8.         -0.725      -0.5       ]]\n",
      "\n",
      " [[-2.00228438 -1.8861875  -1.8549375  -1.809375  ]\n",
      "  [-1.5275     -1.6861875  -1.595      -1.60125   ]\n",
      "  [-1.5280625  -1.20125    -1.2        -0.3125    ]\n",
      "  [-0.975       4.1875     -0.725      -0.5       ]\n",
      "  [-0.5        10.          0.          0.        ]]\n",
      "\n",
      " [[-1.8549375  -1.8549375  -1.8549375  -1.809375  ]\n",
      "  [-1.5275     -1.42625    -1.7924375  -0.0375    ]\n",
      "  [-0.975      -0.975      -1.30125     4.96875   ]\n",
      "  [-0.5        -0.5        -0.725       9.6875    ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q-learning\n",
    "def q_learning():\n",
    "    for episode in range(num_episodes):\n",
    "        state = (0, 0)  # Start state\n",
    "        while state != goal_state:\n",
    "            i, j = state\n",
    "            # Choose action (epsilon-greedy)\n",
    "            action_idx = np.argmax(q_table[i, j])\n",
    "            # Take action\n",
    "            if actions[action_idx] == 'up':\n",
    "                next_i, next_j = max(i - 1, 0), j\n",
    "            elif actions[action_idx] == 'down':\n",
    "                next_i, next_j = min(i + 1, grid_size - 1), j\n",
    "            elif actions[action_idx] == 'left':\n",
    "                next_i, next_j = i, max(j - 1, 0)\n",
    "            elif actions[action_idx] == 'right':\n",
    "                next_i, next_j = i, min(j + 1, grid_size - 1)\n",
    "            # Update Q-value\n",
    "            reward = rewards[next_i, next_j]\n",
    "            q_table[i, j, action_idx] += alpha * (\n",
    "                reward + gamma * np.max(q_table[next_i, next_j]) - q_table[i, j, action_idx]\n",
    "            )\n",
    "            state = (next_i, next_j)\n",
    "\n",
    "# Run Q-learning and print results\n",
    "q_learning()\n",
    "print(\"Learned Q-values:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
